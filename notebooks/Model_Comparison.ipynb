{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# üìä Comparaci√≥n de Modelos de Regresi√≥n\n",
    "\n",
    "En este notebook realizamos una **comparativa final** entre todos los modelos entrenados:  \n",
    "\n",
    "- **Regresi√≥n Lineal**  \n",
    "- **√Årbol de Decisi√≥n**  \n",
    "- **Random Forest**  \n",
    "- **XGBoost (con Optuna)**  \n",
    "- **Ridge, Lasso y Elastic Net**  \n",
    "- **KNN Regressor**\n",
    "\n",
    "La comparaci√≥n se realiza con las m√©tricas:  \n",
    "- **RMSE (Root Mean Squared Error):** penaliza m√°s los errores grandes.  \n",
    "- **MAE (Mean Absolute Error):** mide el error medio absoluto, m√°s robusto a outliers.  \n",
    "- **R¬≤ (Coeficiente de determinaci√≥n):** proporci√≥n de la varianza explicada.  \n",
    "\n",
    "Finalmente, representamos gr√°ficamente las m√©tricas para evaluar qu√© modelo es m√°s adecuado para predecir la **Esperanza de Vida**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## **Paso 1. Importar librer√≠as y cargar resultados**\n",
    "\n",
    "Cargamos los datos de las m√©tricas calculadas para cada algoritmo en su correspondiente notebook:\n",
    "\n",
    "Los rachivos son los siguientes de cada modelo:\n",
    "\n",
    "- df_lr = pd.read_csv(\"../data/results_linear.csv\")\n",
    "- df_trees = pd.read_csv(\"../data/results_trees_all.csv\")\n",
    "- df_xgb_base = pd.read_csv(\"../data/results_xgboost_baseline.csv\")\n",
    "- df_xgb_optuna = pd.read_csv(\"../data/results_xgboost_optuna.csv\")\n",
    "- df_xgb_comp = pd.read_csv(\"../data/results_xgboost_comparison.csv\")  # opcional si quieres baseline vs - optimizado\n",
    "- df_ridge_lasso_en = pd.read_csv(\"../data/results_ridge_lasso_elasticnet.csv\")\n",
    "- df_knn = pd.read_csv(\"../data/results_knn.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m files = glob.glob(\u001b[33m\"\u001b[39m\u001b[33m../data/results_*.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Concatenamos todos los resultados en un √∫nico DataFrame\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m comparison = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Ordenamos por RMSE (menor es mejor)\u001b[39;00m\n\u001b[32m     18\u001b[39m comparison = comparison.sort_values(by=\u001b[33m\"\u001b[39m\u001b[33mRMSE\u001b[39m\u001b[33m\"\u001b[39m).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\P5_G3_Regression\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m op = \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\P5_G3_Regression\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[39m, in \u001b[36m_Concatenator.__init__\u001b[39m\u001b[34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28mself\u001b[39m.verify_integrity = verify_integrity\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m.copy = copy\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m objs, keys = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[32m    448\u001b[39m ndims = \u001b[38;5;28mself\u001b[39m._get_ndims(objs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\P5_G3_Regression\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[39m, in \u001b[36m_Concatenator._clean_keys_and_objs\u001b[39m\u001b[34m(self, objs, keys)\u001b[39m\n\u001b[32m    504\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo objects to concatenate\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    510\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(com.not_none(*objs_list))\n",
      "\u001b[31mValueError\u001b[39m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# ===================================\n",
    "# 1. Importar librer√≠as\n",
    "# ===================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "\n",
    "# ===================================\n",
    "# 2. Cargar todos los resultados de cada modelo autom√°ticamente\n",
    "# ===================================\n",
    "files = glob.glob(\"../data/results_*.csv\")\n",
    "\n",
    "# Concatenamos todos los resultados en un √∫nico DataFrame\n",
    "comparison = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
    "\n",
    "# Ordenamos por RMSE (menor es mejor)\n",
    "comparison = comparison.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
    "\n",
    "print(\"üìä Resultados comparativos consolidados:\")\n",
    "display(comparison)\n",
    "\n",
    "#df_lr = pd.read_csv(\"../data/results_linear.csv\")\n",
    "#df_trees = pd.read_csv(\"../data/results_trees_all.csv\")\n",
    "#df_xgb_base = pd.read_csv(\"../data/results_xgboost_baseline.csv\")\n",
    "#df_xgb_optuna = pd.read_csv(\"../data/results_xgboost_optuna.csv\")\n",
    "#df_xgb_comp = pd.read_csv(\"../data/results_xgboost_comparison.csv\")  # opcional si quieres baseline vs optimizado\n",
    "#df_ridge_lasso_en = pd.read_csv(\"../data/results_ridge_lasso_elasticnet.csv\")\n",
    "#df_knn = pd.read_csv(\"../data/results_knn.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## **Paso 2. Visualizaci√≥n comparativa\n",
    "\n",
    "Los siguientes gr√°ficos muestran la comparaci√≥n entre modelos:  \n",
    "\n",
    "1. **RMSE y MAE:** menor valor = mejor ajuste.  \n",
    "2. **R¬≤:** m√°s cercano a 1 = mejor capacidad explicativa.  \n",
    "\n",
    "Esto nos ayuda a ver r√°pidamente qu√© modelos destacan sobre el resto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================\n",
    "# 3. Comparaci√≥n visual de RMSE y MAE\n",
    "# ===================================\n",
    "plt.figure(figsize=(12,6))\n",
    "comparison.set_index(\"Modelo\")[[\"RMSE\",\"MAE\"]].plot(kind=\"bar\", figsize=(12,6))\n",
    "plt.title(\"Comparaci√≥n de errores (RMSE y MAE) entre modelos\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.legend(title=\"M√©trica\")\n",
    "plt.show()\n",
    "\n",
    "# ===================================\n",
    "# 4. Comparaci√≥n visual de R¬≤\n",
    "# ===================================\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(\n",
    "    x=\"Modelo\", \n",
    "    y=\"R¬≤\", \n",
    "    data=comparison, \n",
    "    palette=\"Blues_r\", \n",
    "    hue=\"Modelo\", \n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"Comparaci√≥n de R¬≤ entre modelos\")\n",
    "plt.ylabel(\"R¬≤\")\n",
    "plt.ylim(0,1)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## **Paso 3. üìä Comparaci√≥n final de modelos**\n",
    "\n",
    "En este bloque consolidamos todos los resultados de los diferentes algoritmos probados:\n",
    "\n",
    "- **Linear Regression**\n",
    "- **Decision Tree**\n",
    "- **Random Forest** (baseline y optimizado)\n",
    "- **XGBoost** (baseline y optimizado con Optuna)\n",
    "- **Ridge, Lasso y Elastic Net**\n",
    "- **KNN Regressor**\n",
    "\n",
    "**Gr√°ficos:**\n",
    "- El primer gr√°fico compara los errores **RMSE** y **MAE** ‚Üí cuanto m√°s bajos, mejor.  \n",
    "- El segundo gr√°fico muestra el **R¬≤** ‚Üí cuanto m√°s alto y m√°s cercano a 1, mejor es la capacidad explicativa del modelo.  \n",
    "\n",
    "Esto nos permite identificar cu√°l modelo tiene mejor desempe√±o global y si hay compromisos (ejemplo: menor error, pero mayor complejidad).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## **Ranking por m√©trica**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 6. Ranking por m√©trica\n",
    "# ===================================\n",
    "\n",
    "# Ranking de los diez mejores modelos por cada m√©trica\n",
    "ranking = pd.DataFrame({\n",
    "    \"Top 10: Mejor RMSE\": comparison.sort_values(\"RMSE\").head(10)[\"Modelo\"].values,\n",
    "    \"Top 10: Mejor MAE\": comparison.sort_values(\"MAE\").head(10)[\"Modelo\"].values,\n",
    "    \"Top 10: Mejor R¬≤\": comparison.sort_values(\"R¬≤\", ascending=False).head(10)[\"Modelo\"].values\n",
    "})\n",
    "\n",
    "print(\"üèÜ Ranking de los diez mejores modelos por m√©trica:\")\n",
    "display(ranking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## **Paso 4. ‚úÖ Conclusiones**\n",
    "\n",
    "- **XGBoost** y **Random Forest** suelen ser los modelos m√°s s√≥lidos para datos tabulares no lineales.  \n",
    "- **Regresi√≥n Lineal** y **Ridge/Lasso/ElasticNet** capturan bien relaciones lineales, pero no modelan interacciones complejas.  \n",
    "- **KNN Regressor** funciona razonablemente, pero es sensible a la escala y a valores at√≠picos.  \n",
    "- La m√©trica m√°s estable y robusta es **MAE**, ya que RMSE penaliza demasiado los outliers.  \n",
    "\n",
    "üìå En este dataset, el modelo que logra el mejor equilibrio entre error bajo (RMSE/MAE) y capacidad explicativa (R¬≤) es **XGBoost**.  \n",
    "\n",
    "Este ser√° el modelo elegido para nuestro **Producto M√≠nimo Viable (PMV)**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
