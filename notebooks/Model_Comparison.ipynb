{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# üìä Comparaci√≥n de Modelos de Regresi√≥n\n",
    "\n",
    "En este notebook realizamos una **comparativa final** entre todos los modelos entrenados:  \n",
    "\n",
    "- **Regresi√≥n Lineal**  \n",
    "- **√Årbol de Decisi√≥n**  \n",
    "- **Random Forest**  \n",
    "- **XGBoost (con Optuna)**  \n",
    "- **Ridge, Lasso y Elastic Net**  \n",
    "- **KNN Regressor**\n",
    "\n",
    "La comparaci√≥n se realiza con las m√©tricas:  \n",
    "- **RMSE (Root Mean Squared Error):** penaliza m√°s los errores grandes.  \n",
    "- **MAE (Mean Absolute Error):** mide el error medio absoluto, m√°s robusto a outliers.  \n",
    "- **R¬≤ (Coeficiente de determinaci√≥n):** proporci√≥n de la varianza explicada.  \n",
    "\n",
    "Finalmente, representamos gr√°ficamente las m√©tricas para evaluar qu√© modelo es m√°s adecuado para predecir la **Esperanza de Vida**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## **Paso 1. Importar librer√≠as y cargar resultados**\n",
    "\n",
    "Cargamos los datos de las m√©tricas calculadas para cada algoritmo en su correspondiente notebook:\n",
    "\n",
    "Los rachivos son los siguientes de cada modelo:\n",
    "\n",
    "- df_lr = pd.read_csv(\"../data/results_linear.csv\")\n",
    "- df_trees = pd.read_csv(\"../data/results_trees_all.csv\")\n",
    "- df_xgb_base = pd.read_csv(\"../data/results_xgboost_baseline.csv\")\n",
    "- df_xgb_optuna = pd.read_csv(\"../data/results_xgboost_optuna.csv\")\n",
    "- df_xgb_comp = pd.read_csv(\"../data/results_xgboost_comparison.csv\")  # opcional si quieres baseline vs - optimizado\n",
    "- df_ridge_lasso_en = pd.read_csv(\"../data/results_ridge_lasso_elasticnet.csv\")\n",
    "- df_knn = pd.read_csv(\"../data/results_knn.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 1. Importar librer√≠as\n",
    "# ===================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "\n",
    "# ===================================\n",
    "# 2. Cargar todos los resultados de cada modelo autom√°ticamente\n",
    "# ===================================\n",
    "files = glob.glob(\"../data/results_*.csv\")\n",
    "\n",
    "# Concatenamos todos los resultados en un √∫nico DataFrame\n",
    "comparison = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
    "\n",
    "# Ordenamos por RMSE (menor es mejor)\n",
    "comparison = comparison.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
    "\n",
    "print(\"üìä Resultados comparativos consolidados:\")\n",
    "display(comparison)\n",
    "\n",
    "#df_lr = pd.read_csv(\"../data/results_linear.csv\")\n",
    "#df_trees = pd.read_csv(\"../data/results_trees_all.csv\")\n",
    "#df_xgb_base = pd.read_csv(\"../data/results_xgboost_baseline.csv\")\n",
    "#df_xgb_optuna = pd.read_csv(\"../data/results_xgboost_optuna.csv\")\n",
    "#df_xgb_comp = pd.read_csv(\"../data/results_xgboost_comparison.csv\")  # opcional si quieres baseline vs optimizado\n",
    "#df_ridge_lasso_en = pd.read_csv(\"../data/results_ridge_lasso_elasticnet.csv\")\n",
    "#df_knn = pd.read_csv(\"../data/results_knn.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## **Paso 2. Visualizaci√≥n comparativa\n",
    "\n",
    "Los siguientes gr√°ficos muestran la comparaci√≥n entre modelos:  \n",
    "\n",
    "1. **RMSE y MAE:** menor valor = mejor ajuste.  \n",
    "2. **R¬≤:** m√°s cercano a 1 = mejor capacidad explicativa.  \n",
    "\n",
    "Esto nos ayuda a ver r√°pidamente qu√© modelos destacan sobre el resto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================\n",
    "# 3. Comparaci√≥n visual de RMSE y MAE\n",
    "# ===================================\n",
    "plt.figure(figsize=(12,6))\n",
    "comparison.set_index(\"Modelo\")[[\"RMSE\",\"MAE\"]].plot(kind=\"bar\", figsize=(12,6))\n",
    "plt.title(\"Comparaci√≥n de errores (RMSE y MAE) entre modelos\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.legend(title=\"M√©trica\")\n",
    "plt.show()\n",
    "\n",
    "# ===================================\n",
    "# 4. Comparaci√≥n visual de R¬≤\n",
    "# ===================================\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(\n",
    "    x=\"Modelo\", \n",
    "    y=\"R¬≤\", \n",
    "    data=comparison, \n",
    "    palette=\"Blues_r\", \n",
    "    hue=\"Modelo\", \n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"Comparaci√≥n de R¬≤ entre modelos\")\n",
    "plt.ylabel(\"R¬≤\")\n",
    "plt.ylim(0,1)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## **Paso 3. üìä Comparaci√≥n final de modelos**\n",
    "\n",
    "En este bloque consolidamos todos los resultados de los diferentes algoritmos probados:\n",
    "\n",
    "- **Linear Regression**\n",
    "- **Decision Tree**\n",
    "- **Random Forest** (baseline y optimizado)\n",
    "- **XGBoost** (baseline y optimizado con Optuna)\n",
    "- **Ridge, Lasso y Elastic Net**\n",
    "- **KNN Regressor**\n",
    "\n",
    "**Gr√°ficos:**\n",
    "- El primer gr√°fico compara los errores **RMSE** y **MAE** ‚Üí cuanto m√°s bajos, mejor.  \n",
    "- El segundo gr√°fico muestra el **R¬≤** ‚Üí cuanto m√°s alto y m√°s cercano a 1, mejor es la capacidad explicativa del modelo.  \n",
    "\n",
    "Esto nos permite identificar cu√°l modelo tiene mejor desempe√±o global y si hay compromisos (ejemplo: menor error, pero mayor complejidad).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## **Ranking por m√©trica**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 6. Ranking por m√©trica\n",
    "# ===================================\n",
    "\n",
    "# Ranking de los tres mejores modelos por cada m√©trica\n",
    "ranking = pd.DataFrame({\n",
    "    \"Top 10: Mejor RMSE\": comparison.sort_values(\"RMSE\").head(10)[\"Modelo\"].values,\n",
    "    \"Top 10: Mejor MAE\": comparison.sort_values(\"MAE\").head(10)[\"Modelo\"].values,\n",
    "    \"Top 10: Mejor R¬≤\": comparison.sort_values(\"R¬≤\", ascending=False).head(10)[\"Modelo\"].values\n",
    "})\n",
    "\n",
    "print(\"üèÜ Ranking de los tres mejores modelos por m√©trica:\")\n",
    "display(ranking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## **Paso 4. ‚úÖ Conclusiones**\n",
    "\n",
    "- **XGBoost** y **Random Forest** suelen ser los modelos m√°s s√≥lidos para datos tabulares no lineales.  \n",
    "- **Regresi√≥n Lineal** y **Ridge/Lasso/ElasticNet** capturan bien relaciones lineales, pero no modelan interacciones complejas.  \n",
    "- **KNN Regressor** funciona razonablemente, pero es sensible a la escala y a valores at√≠picos.  \n",
    "- La m√©trica m√°s estable y robusta es **MAE**, ya que RMSE penaliza demasiado los outliers.  \n",
    "\n",
    "üìå En este dataset, el modelo que logra el mejor equilibrio entre error bajo (RMSE/MAE) y capacidad explicativa (R¬≤) es **XGBoost**.  \n",
    "\n",
    "Este ser√° el modelo elegido para nuestro **Producto M√≠nimo Viable (PMV)**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
