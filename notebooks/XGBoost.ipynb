{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# **XGBoost Regressor**\n",
    "\n",
    "Este notebook tiene como objetivo entrenar, analizar y optimizar un modelo de **XGBoost Regressor** aplicado al dataset de **Esperanza de Vida**.\n",
    "\n",
    "**¬øPor qu√© XGBoost?**  \n",
    "- Es un algoritmo basado en **√°rboles de decisi√≥n** que utiliza la t√©cnica de **boosting** (aprendizaje secuencial), lo que lo hace muy potente para datos tabulares.  \n",
    "- Suele obtener mejores resultados que modelos m√°s simples (como Regresi√≥n Lineal o √Årboles de Decisi√≥n) y maneja bien relaciones no lineales y variables con distintas escalas.  \n",
    "- Incluye regularizaci√≥n para evitar **overfitting**, algo que puede ser un problema en Decision Trees simples.  \n",
    "\n",
    "**Qu√© haremos en este notebook:**  \n",
    "1. Importar librer√≠as y cargar los datasets procesados.  \n",
    "2. Entrenar un modelo **baseline** de XGBoost y evaluar m√©tricas (RMSE, MAE, R¬≤).  \n",
    "3. Analizar **overfitting** mediante curvas de validaci√≥n.  \n",
    "4. Visualizar la **importancia de variables** en el modelo.  \n",
    "5. Optimizar hiperpar√°metros con **Optuna** para mejorar el rendimiento.  \n",
    "6. Comparar el modelo baseline con el optimizado.  \n",
    "7. Concluir sobre la utilidad de XGBoost en este problema y compararlo brevemente con Decision Tree y Random Forest.  \n",
    "\n",
    "Este notebook complementa al de **DecisionTree_RandomForest.ipynb**, profundizando en el uso de un algoritmo de boosting m√°s avanzado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## **Paso 1. Importar librer√≠as**\n",
    "\n",
    "En este bloque importamos todas las librer√≠as necesarias:  \n",
    "\n",
    "- **pandas** para manipulaci√≥n de datos.  \n",
    "- **train_test_split** para dividir el dataset en entrenamiento y validaci√≥n.  \n",
    "- **XGBRegressor** de `xgboost` como modelo principal.  \n",
    "- **m√©tricas de sklearn** para evaluar el rendimiento (RMSE, MAE, R¬≤).  \n",
    "- **matplotlib** y **seaborn** para visualizaciones.  \n",
    "- **numpy** para operaciones matem√°ticas.  \n",
    "\n",
    "Dejamos preparado tambi√©n el entorno para trabajar con **Optuna**, que utilizaremos m√°s adelante en la optimizaci√≥n de hiperpar√°metros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 1. Importar librer√≠as\n",
    "# ===================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "\n",
    "# Modelo principal\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "\n",
    "# Para optimizaci√≥n\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_parallel_coordinate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## **Paso 2. Carga de datos procesados y no escalados**\n",
    "\n",
    "En este bloque importamos las librer√≠as necesarias para entrenar el modelo **XGBoost** \n",
    "y cargamos los datasets procesados.\n",
    "\n",
    "üëâ Usaremos los datos **no escalados**, ya que XGBoost (al igual que Random Forest) \n",
    "no requiere que las variables est√©n normalizadas o estandarizadas.  \n",
    "El dataset ya est√° limpio y preprocesado en el notebook `EDA_processed.ipynb`.  \n",
    "\n",
    "Tambi√©n dividimos los datos en **train** y **validaci√≥n** para evaluar el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 2. Cargar datos procesados y no escalados\n",
    "# ===================================\n",
    "\n",
    "# --- Cargar datasets procesados ---\n",
    "X_ns = pd.read_csv(\"../data/processed/features_no_scaling.csv\")\n",
    "y = pd.read_csv(\"../data/processed/target_y.csv\").squeeze()  # lo convertimos a Serie\n",
    "\n",
    "# Divisi√≥n train / validaci√≥n\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_ns, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Tama√±o entrenamiento:\", X_train.shape, y_train.shape)\n",
    "print(\"Tama√±o validaci√≥n:\", X_valid.shape, y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## **Paso 3. Entrenamiento y evaluaci√≥n del modelo XGBoost (baseline)**\n",
    "\n",
    "En este bloque entrenamos un **XGBoost Regressor** con par√°metros por defecto.  \n",
    "Este modelo pertenece a la familia de **boosting**, es decir, combina √°rboles de decisi√≥n de forma secuencial, donde cada √°rbol corrige los errores del anterior.  \n",
    "\n",
    "Evaluamos las m√©tricas principales:  \n",
    "\n",
    "- **RMSE (Root Mean Squared Error):** mide el error promedio penalizando m√°s los errores grandes.  \n",
    "- **MAE (Mean Absolute Error):** mide el error promedio sin penalizar tanto los valores extremos.  \n",
    "- **R¬≤ (Coeficiente de determinaci√≥n):** indica cu√°nto del comportamiento de la variable objetivo es explicado por el modelo (1 = perfecto, 0 = no explica nada).  \n",
    "\n",
    "Estos valores nos dar√°n una primera referencia de qu√© tan bien se comporta el modelo **sin ajustar hiperpar√°metros**.\n",
    "\n",
    "### üìä Visualizaci√≥n: valores reales vs. predichos (XGBoost baseline)\n",
    "\n",
    "En este gr√°fico comparamos los **valores reales de la esperanza de vida** (eje X) frente a los **valores predichos por el modelo** (eje Y).\n",
    "\n",
    "- La **l√≠nea diagonal en rojo** representa la situaci√≥n ideal: todas las predicciones coinciden exactamente con los valores reales.\n",
    "- Los **puntos azules** son las predicciones reales del modelo.\n",
    "\n",
    "üëâ Interpretaci√≥n:\n",
    "- Cuanto m√°s cerca est√©n los puntos de la l√≠nea roja, mejor est√° funcionando el modelo.\n",
    "- Si los puntos se dispersan mucho lejos de la l√≠nea, significa que el modelo est√° cometiendo errores grandes en esas observaciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 3. Entrenar y evaluar modelo (baseline)\n",
    "# ===================================\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Modelo baseline con par√°metros por defecto\n",
    "xgb = XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=100,   # n√∫mero de √°rboles (default suele ser 100)\n",
    "    learning_rate=0.1,  # tasa de aprendizaje\n",
    "    max_depth=3         # profundidad m√°xima de cada √°rbol\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "preds_xgb = xgb.predict(X_valid)\n",
    "\n",
    "# M√©tricas\n",
    "rmse = np.sqrt(mean_squared_error(y_valid, preds_xgb))\n",
    "mae = mean_absolute_error(y_valid, preds_xgb)\n",
    "r2 = r2_score(y_valid, preds_xgb)\n",
    "\n",
    "print(\"üìä XGBoost (baseline)\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"MAE: {mae:.3f}\")\n",
    "print(f\"R¬≤: {r2:.3f}\")\n",
    "\n",
    "# ===================================\n",
    "# Visualizaci√≥n: valores reales vs. predichos\n",
    "# ===================================\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(x=y_valid, y=preds_xgb, alpha=0.6)\n",
    "plt.plot([y_valid.min(), y_valid.max()], [y_valid.min(), y_valid.max()], color=\"red\", lw=2)\n",
    "plt.xlabel(\"Valores reales\")\n",
    "plt.ylabel(\"Valores predichos\")\n",
    "plt.title(\"XGBoost - Valores reales vs. predichos\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## **Paso 4. Diagn√≥stico de residuos**\n",
    "\n",
    "Antes de evaluar posibles problemas de sobreajuste, analizamos los **residuos del modelo**:\n",
    "\n",
    "- Los **residuos** son las diferencias entre los valores reales (`y_valid`) y las predicciones del modelo (`preds`).\n",
    "- Un buen modelo deber√≠a mostrar residuos **centrados en 0** y sin patrones claros.\n",
    "\n",
    "üëâ En este bloque:\n",
    "1. Graficamos un **Histograma de residuos:** nos muestra c√≥mo se distribuyen los errores.  \n",
    "  - Lo ideal es que los residuos est√©n centrados en 0 y tengan una forma sim√©trica.  \n",
    "  - Una gran asimetr√≠a o colas largas pueden indicar que el modelo falla en ciertos rangos.  \n",
    "\n",
    "2. Graficamos **Dispersi√≥n `y_valid vs. predicciones`:** permite ver si hay **patrones en los errores**, comprobamos si las predicciones siguen bien la diagonal ideal.\n",
    "  - Si los puntos est√°n alineados en torno a la diagonal, significa que el modelo predice bien.  \n",
    "  - Si vemos un patr√≥n en forma de curva o zonas donde el modelo sistem√°ticamente se equivoca (ej. subestima en valores altos), puede haber sesgos o falta de complejidad.  \n",
    "\n",
    "A diferencia de la regresi√≥n lineal, aqu√≠ **no buscamos una relaci√≥n perfectamente lineal**, sino que el modelo capture adecuadamente las variaciones sin dejar patrones evidentes en los residuos.\n",
    " \n",
    "### Interpretaci√≥n:\n",
    "- Si los residuos se concentran alrededor de 0 ‚Üí el modelo generaliza bien.\n",
    "- Si hay colas largas o muchos residuos grandes ‚Üí el modelo est√° fallando en algunos casos.\n",
    "- Si los residuos muestran un patr√≥n (ej. tendencia en forma de curva) ‚Üí puede que falte complejidad o que el modelo est√© sesgado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 4. Diagn√≥stico de residuos (XGBoost)\n",
    "# ===================================\n",
    "\n",
    "# C√°lculo de residuos\n",
    "residuos = y_valid - preds_xgb\n",
    "\n",
    "# --- Gr√°fico 1: valores reales (y_valid) vs predicciones ---\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.scatterplot(x=y_valid, y=preds_xgb, alpha=0.6)\n",
    "plt.plot([y_valid.min(), y_valid.max()],\n",
    "         [y_valid.min(), y_valid.max()],\n",
    "         color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Valores reales (y_valid)\")\n",
    "plt.ylabel(\"Predicciones\")\n",
    "plt.title(\"Predicciones vs. Valores reales (XGBoost)\")\n",
    "plt.show()\n",
    "\n",
    "# --- Gr√°fico 2: Residuos vs predicciones ---\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.scatterplot(x=preds_xgb, y=residuos, alpha=0.6)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Predicciones\")\n",
    "plt.ylabel(\"Residuos\")\n",
    "plt.title(\"Residuos vs. Predicciones (XGBoost)\")\n",
    "plt.show()\n",
    "\n",
    "# --- Gr√°fico 3: Distribuci√≥n de residuos ---\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.histplot(residuos, bins=30, kde=True, color=\"steelblue\")\n",
    "plt.axvline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Residuos: Error (y_valid - predicci√≥n)\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.title(\"Distribuci√≥n de residuos (XGBoost)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## **Paso 5. Overfitting en XGBoost: curva de aprendizaje con max_depth**\n",
    "\n",
    "XGBoost, al igual que los √°rboles de decisi√≥n y Random Forest, puede sobreajustar f√°cilmente si los √°rboles son muy profundos.\n",
    "\n",
    "En este bloque:\n",
    "- Variamos el hiperpar√°metro `max_depth` (profundidad m√°xima de cada √°rbol).\n",
    "- Calculamos el **RMSE en entrenamiento y validaci√≥n** para cada valor.\n",
    "- Graficamos ambas curvas para ver la diferencia.\n",
    "\n",
    "- **Entrenamiento (RMSE Train):** mide qu√© tan bien el modelo memoriza los datos vistos.  \n",
    "- **Validaci√≥n (RMSE Valid):** mide qu√© tan bien generaliza el modelo en datos no vistos.  \n",
    "\n",
    "üìä Interpretaci√≥n:\n",
    "- Si el error de **train** es muy bajo (baja mucho la curva de **entrenamiento**) y el de **validaci√≥n** empeora o dja de mejorar (es muy alto) ‚Üí hay **overfitting**.  \n",
    "- Si ambos errores (o curvas) son altos ‚Üí el modelo est√° **underfitting** (falta de complejidad).  \n",
    "- Lo ideal, el punto √≥ptimo, est√° en encontrar un punto intermedio donde los dos errores (o las dos curvas) sean bajos y cercanos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Paso 5. Overfitting en XGBoost\n",
    "# Curva de aprendizaje con max_depth\n",
    "# ===================================\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "train_errors, valid_errors = [], []\n",
    "depths = range(1, 16)  # probamos de profundidad 1 a 16\n",
    "\n",
    "for d in depths:\n",
    "    model = XGBRegressor(\n",
    "        random_state=42,\n",
    "        n_estimators=200,     # n√∫mero de √°rboles\n",
    "        learning_rate=0.1,    # tama√±o del paso\n",
    "        max_depth=d,\n",
    "        n_jobs=-1          # usar todos los n√∫cleos disponibles\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    preds_train = model.predict(X_train)\n",
    "    preds_valid = model.predict(X_valid)\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, preds_train))\n",
    "    valid_rmse = np.sqrt(mean_squared_error(y_valid, preds_valid))\n",
    "    \n",
    "    train_errors.append(train_rmse)\n",
    "    valid_errors.append(valid_rmse)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(depths, train_errors, label=\"Train RMSE\", marker=\"o\")\n",
    "plt.plot(depths, valid_errors, label=\"Valid RMSE\", marker=\"o\")\n",
    "plt.xlabel(\"Profundidad del √°rbol (max_depth)\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Curva de aprendizaje - Overfitting en XGBoost\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## **Paso 6. Optimizaci√≥n de hiperpar√°metros con Optuna**\n",
    "\n",
    "Hasta ahora hemos probado XGBoost con par√°metros por defecto y variando solo `max_depth`.  \n",
    "Pero este modelo tiene **muchos hiperpar√°metros clave** que afectan al rendimiento y al riesgo de **overfitting**.  \n",
    "\n",
    "Algunos de los m√°s importantes:  \n",
    "- `n_estimators`: n√∫mero de √°rboles que se entrenan (bagging).  \n",
    "- `max_depth`: profundidad m√°xima de cada √°rbol.  \n",
    "- `learning_rate`: tasa de aprendizaje para cada actualizaci√≥n de boosting.  \n",
    "- `subsample`: fracci√≥n de filas que se muestrean al entrenar cada √°rbol.  \n",
    "- `colsample_bytree`: fracci√≥n de columnas (features) que se usan por √°rbol.  \n",
    "- `gamma`: regularizaci√≥n de la divisi√≥n de nodos (controla la complejidad).  \n",
    "- `reg_lambda`: regularizaci√≥n L2 para evitar overfitting.  \n",
    "\n",
    "üëâ Buscar manualmente todos estos par√°metros ser√≠a muy costoso.  \n",
    "Por eso usamos **Optuna**, una librer√≠a de optimizaci√≥n autom√°tica, que explora el espacio de par√°metros y encuentra las combinaciones que minimizan el error de validaci√≥n (aqu√≠ usaremos **RMSE**).  \n",
    "\n",
    "Este paso nos permitir√° obtener un XGBoost mucho m√°s competitivo y comparar con Random Forest de forma justa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 6. Optuna: optimizaci√≥n de hiperpar√°metros\n",
    "# ===================================\n",
    "import optuna\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n",
    "        \"random_state\": 42,\n",
    "        \"eval_metric\": \"rmse\"   # \n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "    \n",
    "    preds = model.predict(X_valid)\n",
    "    rmse = np.sqrt(mean_squared_error(y_valid, preds))\n",
    "    return rmse\n",
    "\n",
    "# Crear estudio de optimizaci√≥n\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)  # n√∫mero de iteraciones (aumentar si tienes tiempo)\n",
    "\n",
    "print(\"üìä Mejores par√°metros encontrados:\")\n",
    "print(study.best_params)\n",
    "print(f\"Mejor RMSE validaci√≥n: {study.best_value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## **Paso 7. Evaluaci√≥n tras la optimizaci√≥n**\n",
    "\n",
    "Evaluaci√≥n del mejor modelo con los par√°metros encontrador por Optuna.\n",
    "\n",
    "Aunque Optuna se ha encargado de **minimizar RMSE**, no nos quedamos solo con esa m√©trica.  \n",
    "Una vez obtenido el mejor conjunto de hiperpar√°metros, volvemos a entrenar el modelo y lo evaluamos con **RMSE, MAE y R¬≤**:\n",
    "\n",
    "- **RMSE**: penaliza m√°s los errores grandes, √∫til para medir precisi√≥n global.  \n",
    "- **MAE**: mide el error medio absoluto, m√°s robusto frente a valores at√≠picos.  \n",
    "- **R¬≤**: proporci√≥n de la varianza de la variable objetivo explicada por el modelo.  \n",
    "\n",
    "Esto nos permite comparar con el resto de algoritmos bajo un mismo criterio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 7. Evaluaci√≥n del mejor modelo Optuna\n",
    "# ===================================\n",
    "\n",
    "best_params = study.best_params\n",
    "best_model = XGBRegressor(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "preds_optuna = best_model.predict(X_valid)\n",
    "\n",
    "# M√©tricas\n",
    "rmse = np.sqrt(mean_squared_error(y_valid, preds_optuna))\n",
    "mae = mean_absolute_error(y_valid, preds_optuna)\n",
    "r2 = r2_score(y_valid, preds_optuna)\n",
    "\n",
    "print(\"üìä XGBoost optimizado con Optuna\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"MAE: {mae:.3f}\")\n",
    "print(f\"R¬≤: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## **Paso 8. Convergencia de Optuna**\n",
    "\n",
    "Optuna permite visualizar c√≥mo evoluciona la m√©trica objetivo (en este caso, **RMSE**) a lo largo de los *trials*.\n",
    "\n",
    "üëâ Si la curva desciende r√°pidamente y luego se estabiliza, significa que el algoritmo encontr√≥ buenos hiperpar√°metros pronto y despu√©s solo afina ligeramente.\n",
    "\n",
    "üëâ Si la curva sigue bajando mucho trial tras trial, quiz√° convendr√≠a aumentar el n√∫mero de *trials*.\n",
    "\n",
    "Este gr√°fico nos ayuda a diagnosticar:\n",
    "- **Rapidez de convergencia** del proceso de optimizaci√≥n.  \n",
    "- Si los par√°metros √≥ptimos encontrados son estables o podr√≠an mejorar con m√°s iteraciones.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 8. Visualizaci√≥n de convergencia de Optuna\n",
    "# ===================================\n",
    "\n",
    "\n",
    "fig = plot_optimization_history(study)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## **Interpretaci√≥n del gr√°fico de convergencia de Optuna**\n",
    "\n",
    "El gr√°fico de **convergencia de Optuna** nos muestra c√≥mo evoluciona el error del modelo (RMSE en validaci√≥n) a medida que se prueban diferentes combinaciones de hiperpar√°metros.\n",
    "\n",
    "\n",
    "\n",
    "###  ¬øQu√© representa?\n",
    "- **Eje X (iteraciones / trials):** cada prueba realizada con un conjunto distinto de hiperpar√°metros.  \n",
    "- **Eje Y (RMSE en validaci√≥n):** el valor de la m√©trica que tratamos de minimizar en cada trial.  \n",
    "- **Puntos / l√≠nea azul:** error alcanzado en cada prueba.  \n",
    "- **L√≠nea roja (si aparece):** el mejor valor encontrado hasta ese momento (incumbent curve).  \n",
    "\n",
    "\n",
    "###  C√≥mo interpretarlo\n",
    "1. **Descenso r√°pido inicial** ‚Üí significa que Optuna encontr√≥ enseguida par√°metros mucho mejores que los iniciales.  \n",
    "2. **Meseta despu√©s de varios trials** ‚Üí indica que el modelo ya alcanz√≥ un buen rendimiento y no mejora con m√°s pruebas.  \n",
    "3. **Picos hacia arriba en algunos trials** ‚Üí normales, Optuna explora combinaciones que a veces rinden peor.  \n",
    "\n",
    "\n",
    "\n",
    "###  Qu√© nos dice sobre nuestro modelo\n",
    "- Si la curva converge r√°pido ‚Üí el modelo es **estable y f√°cil de ajustar**.  \n",
    "- Si mejora poco a poco tras muchos trials ‚Üí puede valer la pena aumentar `n_trials`.  \n",
    "- Si la curva nunca baja mucho ‚Üí el modelo tiene limitaciones estructurales (no basta con ajustar par√°metros).  \n",
    "\n",
    "En nuestro caso, Optuna alcanz√≥ un **RMSE ‚âà 1.565**, lo que indica un buen ajuste de hiperpar√°metros. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## **Paso 9. Importancia de hiperpar√°metros con Optuna**\n",
    "\n",
    "Adem√°s de encontrar los mejores valores, Optuna nos permite analizar **qu√© hiperpar√°metros fueron m√°s relevantes** para reducir el error del modelo.  \n",
    "\n",
    "- Esta visualizaci√≥n muestra **la contribuci√≥n relativa de cada hiperpar√°metro** en el rendimiento (RMSE).  \n",
    "- Cuanto m√°s alta sea la importancia, m√°s influy√≥ ese par√°metro en el resultado.  \n",
    "- Esto nos ayuda a entender **qu√© par√°metros merecen m√°s atenci√≥n** en futuros ajustes.  \n",
    "\n",
    "\n",
    "**Significado de los principales hiperpar√°metros de XGBoost**\n",
    "\n",
    "Optuna busc√≥ los mejores valores para los siguientes hiperpar√°metros:\n",
    "\n",
    "- **n_estimators** ‚Üí n√∫mero de √°rboles en el ensemble.  \n",
    "  - Valores altos aumentan la capacidad del modelo, pero tambi√©n el tiempo de entrenamiento y riesgo de sobreajuste.  \n",
    "\n",
    "- **max_depth** ‚Üí profundidad m√°xima de cada √°rbol.  \n",
    "  - √Årboles m√°s profundos capturan interacciones complejas, pero pueden sobreajustar.  \n",
    "\n",
    "- **learning_rate (eta)** ‚Üí cu√°nto se corrige cada nuevo √°rbol respecto al error anterior.  \n",
    "  - Valores bajos hacen que el modelo aprenda m√°s despacio pero de forma m√°s estable.  \n",
    "\n",
    "- **subsample** ‚Üí fracci√≥n de muestras que usa cada √°rbol al entrenarse.  \n",
    "  - Reduce sobreajuste al introducir aleatoriedad (ejemplo: 0.8 = usa el 80% de los datos en cada √°rbol).  \n",
    "\n",
    "- **colsample_bytree** ‚Üí fracci√≥n de variables que se usan en cada √°rbol.  \n",
    "  - Similar al subsample, pero a nivel de columnas (features).  \n",
    "\n",
    "- **gamma** ‚Üí penalizaci√≥n m√≠nima de reducci√≥n de p√©rdida para hacer una partici√≥n.  \n",
    "  - Valores altos generan √°rboles m√°s conservadores (menos divisiones).  \n",
    "\n",
    "- **reg_lambda (L2 regularization)** ‚Üí fuerza de la regularizaci√≥n en los pesos de los √°rboles.  \n",
    "  - Ayuda a reducir sobreajuste y estabilizar el modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 9. Importancia de hiperpar√°metros con Optuna\n",
    "# ===================================\n",
    "from optuna.visualization import plot_param_importances\n",
    "\n",
    "fig = plot_param_importances(study)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## **Paso 10. Importancia de las variables en XGBoost**\n",
    "\n",
    "Al igual que Random Forest, XGBoost nos permite medir **qu√© variables aportan m√°s informaci√≥n para reducir el error**.  \n",
    "Esto se calcula a partir de la frecuencia e impacto con que una variable aparece en los nodos de decisi√≥n.\n",
    "\n",
    "El gr√°fico mostrar√° las **20 variables m√°s influyentes** en la predicci√≥n de la esperanza de vida.\n",
    "\n",
    "\n",
    "## üìä Interpretaci√≥n del gr√°fico de importancia de variables  \n",
    "\n",
    "El gr√°fico muestra las **20 variables m√°s influyentes** en el modelo XGBoost:  \n",
    "\n",
    "- **Eje Y** ‚Üí nombre de las variables predictoras.  \n",
    "- **Eje X** ‚Üí valor de importancia asignado por el modelo.  \n",
    "- **Barras m√°s largas** ‚Üí indican que esa variable contribuy√≥ m√°s a reducir el error en los √°rboles.  \n",
    "\n",
    "**C√≥mo leerlo:**  \n",
    "1. Las primeras variables del ranking son las que el modelo us√≥ con m√°s frecuencia y mayor efecto en las divisiones.  \n",
    "2. Variables con barras cortas tuvieron un impacto mucho menor en las predicciones.  \n",
    "3. La comparaci√≥n permite identificar cu√°les son los **principales factores asociados a la esperanza de vida** en este dataset.  \n",
    "\n",
    "**Qu√© nos dice:**  \n",
    "- Permite detectar patrones: por ejemplo, si ‚ÄúAdult Mortality‚Äù o ‚ÄúGDP‚Äù aparecen arriba, sabemos que son variables determinantes.  \n",
    "- Ofrece informaci√≥n **explicable y accionable**, √∫til para responsables de pol√≠ticas de salud o an√°lisis econ√≥mico.  \n",
    "\n",
    "\n",
    "## üîé Diferencias en la importancia de variables entre XGBoost y Random Forest\n",
    "\n",
    "Aunque ambos modelos est√°n basados en **√°rboles de decisi√≥n**, el c√°lculo de la importancia de las variables no es id√©ntico:\n",
    "\n",
    "1. **Random Forest (bagging):**  \n",
    "   - Promedia muchos √°rboles independientes entrenados sobre subconjuntos aleatorios de datos y variables.  \n",
    "   - La importancia se calcula seg√∫n cu√°nto reduce la **impureza (varianza)** en los nodos a lo largo de todos los √°rboles.  \n",
    "   - Tiende a repartir importancia entre variables correlacionadas.\n",
    "\n",
    "2. **XGBoost (boosting):**  \n",
    "   - Construye los √°rboles de forma **secuencial**, corrigiendo errores de predicciones anteriores.  \n",
    "   - Da m√°s importancia a las variables que ayudan a mejorar el ajuste en cada iteraci√≥n.  \n",
    "   - Puede concentrar la relevancia en unas pocas variables clave y dejar otras con menor peso.\n",
    "\n",
    "üìä **Por qu√© no coinciden las top 20 variables:**  \n",
    "- Cada modelo detecta y prioriza relaciones distintas.  \n",
    "- Con variables correlacionadas, Random Forest reparte importancia, mientras que XGBoost selecciona la m√°s eficaz para reducir error.  \n",
    "- Esto no significa que uno sea ‚Äúmejor‚Äù que otro, sino que reflejan estrategias distintas.\n",
    "\n",
    "üí° **Conclusi√≥n:**  \n",
    "Las diferencias en las variables m√°s importantes son **normales**. Ambos modelos coinciden en gran parte en las variables clave, pero difieren en el orden y en c√≥mo asignan la importancia.  \n",
    "Esto refleja la naturaleza distinta de los m√©todos de **bagging (Random Forest)** y **boosting (XGBoost)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 10. Importancia de variables en XGBoost (agrupadas)\n",
    "# ===================================\n",
    "\n",
    "def clean_feature_name(name):\n",
    "    \"\"\"Agrupa dummies de Country y Status en sus variables originales\"\"\"\n",
    "    if name.startswith(\"Country_\"):\n",
    "        return \"Country\"\n",
    "    if name.startswith(\"Status_\"):\n",
    "        return \"Status\"\n",
    "    return name\n",
    "\n",
    "# DataFrame de importancias\n",
    "importances = pd.DataFrame({\n",
    "    \"Variable\": X_train.columns,\n",
    "    \"Importancia\": model.feature_importances_\n",
    "})\n",
    "\n",
    "# Reagrupar importancias\n",
    "importances[\"Variable_grouped\"] = importances[\"Variable\"].apply(clean_feature_name)\n",
    "grouped = (\n",
    "    importances.groupby(\"Variable_grouped\")[\"Importancia\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .sort_values(by=\"Importancia\", ascending=False)\n",
    ")\n",
    "\n",
    "# Visualizaci√≥n top 20\n",
    "plt.figure(figsize=(8,10))\n",
    "sns.barplot(\n",
    "    x=\"Importancia\", \n",
    "    y=\"Variable_grouped\", \n",
    "    data=grouped.head(20),\n",
    "    hue=\"Importancia\", \n",
    "    dodge=False, \n",
    "    legend=False, \n",
    "    palette=\"Blues_r\"\n",
    ")\n",
    "plt.title(\"Top 20 variables m√°s importantes (XGBoost agrupado)\")\n",
    "plt.xlabel(\"Importancia\")\n",
    "plt.ylabel(\"Variable\")\n",
    "plt.show()\n",
    "\n",
    "# Mostrar tabla\n",
    "display(grouped.head(20))\n",
    "\n",
    "# Convertir top 10 a lista\n",
    "top_features = grouped.head(10)[\"Variable_grouped\"].tolist()\n",
    "print(\"Top 10 variables agrupadas:\", top_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## **Paso 11. Comparaci√≥n: baseline vs. modelo optimizado**\n",
    "\n",
    "Ya tenemos dos versiones del modelo:\n",
    "\n",
    "1. **XGBoost baseline** ‚Üí Entrenado con hiperpar√°metros por defecto.  \n",
    "2. **XGBoost optimizado** ‚Üí Ajustado con Optuna para minimizar el RMSE.  \n",
    "\n",
    "Aqu√≠ comparamos ambas versiones en las tres m√©tricas (RMSE, MAE y R¬≤) para ver si la optimizaci√≥n realmente mejor√≥ el rendimiento.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 11. Comparaci√≥n baseline vs. optimizado\n",
    "# ===================================\n",
    "\n",
    "# --- Baseline ---\n",
    "xgb_base = XGBRegressor(random_state=42)\n",
    "xgb_base.fit(X_train, y_train)\n",
    "preds_base = xgb_base.predict(X_valid)\n",
    "\n",
    "baseline_results = {\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(y_valid, preds_base)),\n",
    "    \"MAE\": mean_absolute_error(y_valid, preds_base),\n",
    "    \"R2\": r2_score(y_valid, preds_base)\n",
    "}\n",
    "\n",
    "# --- Optimizado con Optuna ---\n",
    "xgb_best = XGBRegressor(**study.best_params, random_state=42)\n",
    "xgb_best.fit(X_train, y_train)\n",
    "preds_best = xgb_best.predict(X_valid)\n",
    "\n",
    "optimized_results = {\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(y_valid, preds_best)),\n",
    "    \"MAE\": mean_absolute_error(y_valid, preds_best),\n",
    "    \"R2\": r2_score(y_valid, preds_best)\n",
    "}\n",
    "\n",
    "# --- Comparaci√≥n ---\n",
    "comparison_df = pd.DataFrame([baseline_results, optimized_results], \n",
    "                             index=[\"Baseline\", \"Optimizado\"])\n",
    "\n",
    "print(\"üìä Comparaci√≥n de XGBoost baseline vs. optimizado:\\n\")\n",
    "display(comparison_df.style.format({\"RMSE\": \"{:.3f}\", \"MAE\": \"{:.3f}\", \"R2\": \"{:.3f}\"})\n",
    "        .background_gradient(cmap=\"Blues\"))\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# Funci√≥n para guardar m√©tricas estandarizadas\n",
    "# ===================================\n",
    "def save_results(model_name, y_true, y_pred, filepath):\n",
    "    \"\"\"\n",
    "    Calcula m√©tricas (RMSE, MAE, R¬≤) y guarda en CSV estandarizado.\n",
    "    \"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    results = pd.DataFrame([{\n",
    "        \"Modelo\": model_name,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"R¬≤\": r2\n",
    "    }])\n",
    "\n",
    "    results.to_csv(filepath, index=False)\n",
    "    print(f\"‚úÖ Resultados de {model_name} guardados en {filepath}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# Guardar resultados en CSV\n",
    "# ===================================\n",
    "\n",
    "# ===================================\n",
    "# Funci√≥n para guardar m√©tricas estandarizadas\n",
    "# ===================================\n",
    "def save_results(model_name, y_true, y_pred, filepath):\n",
    "    \"\"\"\n",
    "    Calcula m√©tricas (RMSE, MAE, R¬≤) y guarda en CSV estandarizado.\n",
    "    \"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    results = pd.DataFrame([{\n",
    "        \"Modelo\": model_name,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"R¬≤\": r2\n",
    "    }])\n",
    "\n",
    "    results.to_csv(filepath, index=False)\n",
    "    print(f\"‚úÖ Resultados de {model_name} guardados en {filepath}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# Guardar m√©tricas XGBoost (baseline)\n",
    "# ===================================\n",
    "results_xgb_base = pd.DataFrame([{\n",
    "    \"Modelo\": \"XGBoost (baseline)\",\n",
    "    \"RMSE\": baseline_results[\"RMSE\"],\n",
    "    \"MAE\": baseline_results[\"MAE\"],\n",
    "    \"R¬≤\": baseline_results[\"R2\"]\n",
    "}])\n",
    "\n",
    "results_xgb_base.to_csv(\"../data/results_xgboost_baseline.csv\", index=False)\n",
    "\n",
    "print(\"üìä XGBoost (baseline)\")\n",
    "print(f\"RMSE: {baseline_results['RMSE']:.3f}\")\n",
    "print(f\"MAE: {baseline_results['MAE']:.3f}\")\n",
    "print(f\"R¬≤: {baseline_results['R2']:.3f}\")\n",
    "print(\"‚úÖ Resultados guardados en data/results_xgboost_baseline.csv\")\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# Guardar m√©tricas XGBoost (Optuna / optimizado)\n",
    "# ===================================\n",
    "results_xgb_optuna = pd.DataFrame([{\n",
    "    \"Modelo\": \"XGBoost (optuna)\",\n",
    "    \"RMSE\": optimized_results[\"RMSE\"],\n",
    "    \"MAE\": optimized_results[\"MAE\"],\n",
    "    \"R¬≤\": optimized_results[\"R2\"]\n",
    "}])\n",
    "\n",
    "results_xgb_optuna.to_csv(\"../data/results_xgboost_optuna.csv\", index=False)\n",
    "\n",
    "print(\"üìä XGBoost (optuna)\")\n",
    "print(f\"RMSE: {optimized_results['RMSE']:.3f}\")\n",
    "print(f\"MAE: {optimized_results['MAE']:.3f}\")\n",
    "print(f\"R¬≤: {optimized_results['R2']:.3f}\")\n",
    "print(\"‚úÖ Resultados guardados en data/results_xgboost_optuna.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## **Paso 12. Conclusiones finales sobre XGBoost**\n",
    "\n",
    "Tras entrenar y evaluar XGBoost en este dataset de esperanza de vida, podemos resumir:\n",
    "\n",
    "- **Baseline**: el modelo ya ofrece un rendimiento s√≥lido gracias a su naturaleza de boosting, que combina m√∫ltiples √°rboles secuenciales.  \n",
    "- **Optimizaci√≥n con Optuna**: los hiperpar√°metros ajustados permitieron reducir el error (RMSE y MAE) y mejorar el R¬≤.  \n",
    "  - Esto significa que el modelo se adapta mejor a la complejidad de los datos sin sobreajustarse.  \n",
    "- **Interpretaci√≥n del gr√°fico de convergencia**: observamos que el RMSE fue descendiendo en las sucesivas pruebas de Optuna hasta estabilizarse en un valor cercano al m√≠nimo. Esto nos confirma que la b√∫squeda fue efectiva.  \n",
    "- **Comparaci√≥n global**:  \n",
    "  - XGBoost supera a un √°rbol de decisi√≥n simple, que tiende a sobreajustar.  \n",
    "  - Tambi√©n mejora a Random Forest en este dataset, ya que el boosting maneja mejor las interacciones complejas y reduce el sesgo.  \n",
    "  - Frente a modelos lineales (como Regresi√≥n Lineal o Ridge), XGBoost tiene ventaja porque los datos no siguen relaciones puramente lineales.\n",
    "\n",
    "‚úÖ **Conclusi√≥n principal**:  \n",
    "XGBoost optimizado es el modelo m√°s robusto y preciso en este proyecto, y se justifica usarlo como **modelo final para el PMV**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìå Conclusiones:\")\n",
    "print(\"- Baseline XGBoost ya era competitivo, pero tras optimizaci√≥n baj√≥ el RMSE y subi√≥ el R¬≤.\")\n",
    "print(\"- Optuna encontr√≥ hiperpar√°metros que reducen sobreajuste y capturan mejor la relaci√≥n entre variables.\")\n",
    "print(\"- Frente a Decision Tree y Random Forest, XGBoost se adapta mejor a la complejidad del dataset.\")\n",
    "print(\"- Frente a modelos lineales, maneja relaciones no lineales y m√∫ltiples interacciones de forma m√°s eficaz.\")\n",
    "print(\"üëâ Modelo final recomendado: XGBoost optimizado.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
