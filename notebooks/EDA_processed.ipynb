{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# **Exploratory Data Analysis (EDA) & Preprocessing**\n",
    "Este notebook contiene el an√°lisis exploratorio de datos (EDA) y el preprocesamiento del dataset **Life Expectancy**.  \n",
    "El objetivo es preparar los datos para ser usados en los distintos modelos de regresi√≥n.\n",
    "\n",
    "En este notebook, se cargan los datos, se hace exploraci√≥n inicial, tratamiento de nulos, outliers, codificaci√≥n, escalado y exportaci√≥n de datasets listos (X_scaled, X_no_scaled, target_y)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## **Paso 1. Cargar librer√≠as**\n",
    "Importamos las librer√≠as de an√°lisis y visualizaci√≥n que vamos a usar:\n",
    "\n",
    "* pandas ‚Üí manipulaci√≥n de datos.\n",
    "* matplotlib/seaborn ‚Üí visualizaciones est√°ticas.\n",
    "* plotly ‚Üí gr√°ficos interactivos.\n",
    "* sklean ‚Üí herramientas ML y DS.\n",
    "* missingno ‚Üí visualizaci√≥n de datos nulos\n",
    "* StandardScaler from scikit learn.preprocessing ‚Üí Escalado de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 1. Importaci√≥n de librer√≠as\n",
    "# ===================================\n",
    "\n",
    "import pandas as pd                                                             # Manipulaci√≥n de datos y an√°lisis\n",
    "import numpy as np                                                              # C√°lculos num√©ricos                               \n",
    "import matplotlib.pyplot as plt                                                 # Gr√°ficos b√°sicos (histogramas, scatter, etc.).\n",
    "import seaborn as sns                                                           # Gr√°ficos estad√≠sticos con menos c√≥digo (heatmaps, distribuciones‚Ä¶)\n",
    "import missingno as msno                                                        # Visualizaci√≥n de datos faltantes\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler                  # Escalado de variables (normalizaci√≥n y estandarizaci√≥n)\n",
    "\n",
    "\n",
    "# Configuraci√≥n de estilo\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams.update({\"figure.figsize\": (8,5), \"axes.titlesize\": 14, \"axes.labelsize\": 12})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## **Paso 2. Carga del dataset (../data/life_expectancy_data.csv)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 2. Cargar dataset original\n",
    "# (el archivo debe estar en la carpeta data/)\n",
    "# ===================================\n",
    "\n",
    "df = pd.read_csv(\"../data/life_expectancy_data.csv\")\n",
    "\n",
    "# Vista general\n",
    "print(\"Dimensiones:\", df.shape)\n",
    "display(df.head(10))  # üëà muestra primeras 10 filas en formato tabla\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## **Paso 3. EDA inicial: Informaci√≥n b√°sica**\n",
    "\n",
    "En este bloque realizamos una **exploraci√≥n general del dataset**:\n",
    "- Revisamos las primeras filas para entender la estructura de los datos.\n",
    "- Comprobamos el n√∫mero de filas y columnas.\n",
    "- Inspeccionamos el tipo de variables (num√©ricas y categ√≥ricas).\n",
    "- Verificamos si existen valores nulos en las columnas.\n",
    "\n",
    "Este paso es clave para familiarizarnos con el dataset y planificar los siguientes pasos de limpieza y preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 3. EDA inicial: Informaci√≥n b√°sica\n",
    "# ===================================\n",
    "\n",
    "# Revisar nombres de columnas\n",
    "print(\"\\nInformaci√≥n del dataset:\\n\")\n",
    "df.info()\n",
    "\n",
    "# Separador visual\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# Limpiar espacios en nombres de columnas\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# Valores nulos\n",
    "# ===================================\n",
    "print(\"\\nValores nulos por columna (top 10):\\n\")\n",
    "print(df.isnull().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# Visualizaci√≥n de nulos con missingno\n",
    "# ===================================\n",
    "\n",
    "# Mapa de calor de nulos\n",
    "msno.matrix(df, figsize=(10, 5), color=(0.2, 0.4, 0.7))\n",
    "plt.title(\"Mapa de calor de valores nulos\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Definir variable objetivo\n",
    "target_col = \"Life expectancy\"  \n",
    "\n",
    "# Distribuci√≥n de la variable objetivo\n",
    "sns.histplot(df[target_col], kde=True, color=\"blue\")\n",
    "plt.title(\"Distribuci√≥n de la variable objetivo (Life Expectancy)\")\n",
    "plt.show()\n",
    "\n",
    "# Separador visual\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## **Paso 4. Correlaciones iniciales**\n",
    "\n",
    "En este bloque analizamos las **relaciones lineales** entre las variables num√©ricas del dataset.  \n",
    "1. Primero generamos un **mapa de calor** (*heatmap*) para visualizar todas las correlaciones entre pares de variables.  \n",
    "   - Esto nos permite detectar posibles redundancias entre predictores (multicolinealidad).  \n",
    "2. Despu√©s calculamos y mostramos las correlaciones de **cada variable con la variable objetivo** (`Life expectancy`).  \n",
    "   - De esta forma identificamos qu√© variables est√°n m√°s asociadas con la esperanza de vida y cu√°les aportan menos informaci√≥n.  \n",
    "3. Finalmente graficamos estas correlaciones en un **gr√°fico de barras ordenado**, lo que facilita interpretar qu√© predictores tienen m√°s peso potencial en el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 4. Correlaciones iniciales num√©ricas y correlaciones con la variable objetivo\n",
    "# ===================================\n",
    "\n",
    "\n",
    "# Seleccionar num√©ricas\n",
    "df_num = df.select_dtypes(include=[\"int64\", \"float64\"])\n",
    "\n",
    "# --- Heatmap de correlaciones num√©ricas\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    df_num.corr(),\n",
    "    cmap=\"coolwarm\",\n",
    "    annot=True,         #  muestra los valores dentro de cada celda\n",
    "    fmt=\".2f\",          #  formato con 2 decimales\n",
    "    annot_kws={\"size\": 8}  #  tama√±o del texto\n",
    ")\n",
    "plt.title(\"Mapa de calor de correlaciones num√©ricas\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# --- Correlaciones con la variable objetivo\n",
    "correlations = df_num.corr()[target_col].sort_values(ascending=False)\n",
    "\n",
    "print(\"üìä Correlaciones con la variable objetivo:\")\n",
    "print(correlations)\n",
    "\n",
    "# Visualizaci√≥n en gr√°fico de barras\n",
    "plt.figure(figsize=(8, 10))\n",
    "sns.barplot(\n",
    "    x=correlations.values,\n",
    "    y=correlations.index,\n",
    "    hue=correlations.index,  # usar cada variable como hue\n",
    "    palette=\"coolwarm\",\n",
    "    dodge=False,\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "\n",
    "plt.title(f\"Correlaci√≥n de variables num√©ricas con {target_col}\", fontsize=14)\n",
    "plt.xlabel(\"Coeficiente de correlaci√≥n (Pearson)\")\n",
    "plt.ylabel(\"Variables\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## **Paso 5: Outliers (boxplots + IQR)**\n",
    "\n",
    "Los **outliers** son valores que se alejan mucho del resto de observaciones y pueden distorsionar las m√©tricas de entrenamiento.  \n",
    "En este bloque aplicamos dos t√©cnicas principales:\n",
    "\n",
    "1. **Boxplots (diagramas de caja y bigotes)**:  \n",
    "   - Permiten identificar valores extremos en la distribuci√≥n de cada variable num√©rica.  \n",
    "   - Los puntos fuera de los bigotes suelen considerarse outliers.\n",
    "\n",
    "2. **M√©todo del IQR (Interquartile Range)**:  \n",
    "   - Calculamos los percentiles Q1 (25%) y Q3 (75%).  \n",
    "   - Definimos un rango intercuart√≠lico (IQR = Q3 - Q1).  \n",
    "   - Todo valor < Q1 - 1.5√óIQR o > Q3 + 1.5√óIQR se considera un outlier.  \n",
    "\n",
    "Este an√°lisis nos ayuda a decidir si necesitamos **tratar o eliminar** los valores extremos antes de entrenar los modelos.\n",
    "  \n",
    "En general:\n",
    "- Algoritmos basados en distancias o regresi√≥n lineal son sensibles a outliers.  \n",
    "- Los √°rboles de decisi√≥n y ensembles (Random Forest, XGBoost) son m√°s robustos a su presencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 5. Outliers con IQR\n",
    "# ===================================\n",
    "\n",
    "# --- Boxplots para variables num√©ricas\n",
    "num_cols = df.select_dtypes(include=[\"float64\",\"int64\"]).columns\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=df_num, orient=\"h\", palette=\"Set2\", showfliers=True)\n",
    "plt.title(\"Boxplots de variables num√©ricas\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "for col in num_cols[:5]:  # mostramos solo primeras 5 para no saturar\n",
    "    sns.boxplot(x=df[col], color=\"skyblue\")\n",
    "    plt.title(f\"Boxplot de {col}\")\n",
    "    plt.show()\n",
    "\n",
    "# Detectar outliers con m√©todo IQR\n",
    "def detect_outliers_iqr(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "    return series[(series < lower) | (series > upper)]\n",
    "\n",
    "outliers_count = {}\n",
    "for col in num_cols:\n",
    "    outliers_count[col] = len(detect_outliers_iqr(df[col].dropna()))\n",
    "\n",
    "print(\"N√∫mero de outliers detectados por variable:\")\n",
    "print(pd.Series(outliers_count).sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## **Paso 6: Preprocesamiento**\n",
    "\n",
    "En este bloque transformamos el dataset para que pueda ser utilizado por distintos modelos de machine learning:\n",
    "\n",
    "1. Eliminamos columnas con demasiados valores nulos (>40%-50%) y que aquellas que no aporten nada (ej. identificadores, c√≥digos sin significado).\n",
    "2. Rellenamos valores faltantes:  \n",
    "   - Num√©ricas: con la mediana (robusta a outliers).  \n",
    "   - Categ√≥ricas: con la categor√≠a `\"Unknown\"`.  \n",
    "3. Reducimos categor√≠as poco frecuentes en variables categ√≥ricas (ej. pa√≠ses con pocas observaciones) agrup√°ndolas como `\"Other\"`.  \n",
    "4. Aplicamos **One-Hot Encoding** a las variables categ√≥ricas ‚Üí convierte categor√≠as en columnas binarias (0/1).  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "üí° **Nota: Tratamiento de valores nulos en la variable objetivo (Life expectancy)**\n",
    "\n",
    "Durante el preprocesamiento detectamos que la variable objetivo `Life expectancy` conten√≠a **10 valores nulos**.\n",
    "\n",
    "Esto es importante porque un modelo de Machine Learning **no puede entrenar** sin la variable objetivo (`y`).\n",
    "\n",
    "Opciones posibles:\n",
    "\n",
    "1. **Eliminar filas con NaN en `y` (recomendado)**  \n",
    "   - Justificaci√≥n: al ser solo 10 de 2938 filas (~0.3%), el impacto es m√≠nimo.  \n",
    "   - Evitamos introducir sesgo artificial.  \n",
    "\n",
    "2. **Imputar `y` con media o mediana (no recomendado)**  \n",
    "   - Se puede hacer, pero a√±ade un valor inventado que no estaba en el dataset original.  \n",
    "   - Puede distorsionar la distribuci√≥n y las m√©tricas del modelo.\n",
    "\n",
    " En nuestro caso aplicamos la **opci√≥n 1 (eliminaci√≥n de filas con NaN en `y`)**, quedando finalmente **2928 registros v√°lidos**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 6. Preprocesamiento\n",
    "# ===================================\n",
    "\n",
    "# --- Separar target ---\n",
    "y = df[target_col]\n",
    "df_features = df.drop(columns=[target_col])\n",
    "\n",
    "# --- Eliminar columnas con m√°s del 40% de nulos ---\n",
    "missing = df_features.isnull().mean()\n",
    "cols_to_drop = missing[missing > 0.4].index\n",
    "df_features = df_features.drop(columns=cols_to_drop)\n",
    "\n",
    "# --- Rellenar nulos ---\n",
    "for col in df_features.select_dtypes(include=[\"int64\",\"float64\"]).columns:\n",
    "    df_features[col] = df_features[col].fillna(df_features[col].median())\n",
    "\n",
    "for col in df_features.select_dtypes(include=[\"object\"]).columns:\n",
    "    df_features[col] = df_features[col].fillna(\"Unknown\")\n",
    "\n",
    "# --- Reducir categor√≠as (si >20 √∫nicas ‚Üí agrupar en \"Other\") ---\n",
    "for col in df_features.select_dtypes(include=[\"object\"]).columns:\n",
    "    if df_features[col].nunique() > 20:\n",
    "        top = df_features[col].value_counts().index[:20]\n",
    "        df_features[col] = df_features[col].apply(lambda x: x if x in top else \"Other\")\n",
    "\n",
    "# --- One-hot encoding ---\n",
    "df_features = pd.get_dummies(df_features, drop_first=True)\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# Tratamiento de nulos en y\n",
    "# ===================================\n",
    "\n",
    "#  --- LIMPIEZA adicional de la variable objetivo ---\n",
    "print(f\"Valores nulos antes en la variable objetivo y ({target_col}):\", y.isna().sum())\n",
    "\n",
    "# Eliminar filas con y nulo\n",
    "mask = y.notna()\n",
    "df_features = df_features.loc[mask].reset_index(drop=True)\n",
    "y = y.loc[mask].reset_index(drop=True)\n",
    "\n",
    "print(\"Valores nulos en y (despu√©s):\", y.isna().sum())\n",
    "print(\"Dimensiones finales despu√©s de limpiar target:\", df_features.shape, y.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## **Paso 7: Escalado y exportaci√≥n**\n",
    "\n",
    "1. Creamos dos versiones del dataset:  \n",
    "   - `X_no_scaling`: para modelos robustos a la escala (√°rboles, Random Forest, XGBoost).  \n",
    "   - `X_scaled`: aplicamos `StandardScaler`, necesario en modelos sensibles a magnitud (Regresi√≥n Lineal, KNN, SVM).\n",
    "   - `target_y`: dataframe con la variable objetivo `Life expectancy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 7. Escalado y exportaci√≥n\n",
    "# ===================================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(df_features), columns=df_features.columns)\n",
    "\n",
    "# --- Comprobaci√≥n final de nulos ---\n",
    "print(\"üîé Comprobaci√≥n final de nulos en datasets procesados:\")\n",
    "print(\" - X_no_scaling:\", df_features.isna().sum().sum())\n",
    "print(\" - X_scaled:\", X_scaled.isna().sum().sum())\n",
    "print(\" - y:\", y.isna().sum())\n",
    "\n",
    "# Guardar datasets\n",
    "df_features.to_csv(\"../data/processed/features_no_scaling.csv\", index=False)\n",
    "X_scaled.to_csv(\"../data/processed/features_scaled.csv\", index=False)\n",
    "y.to_csv(\"../data/processed/target_y.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Datasets procesados guardados en data/processed/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
